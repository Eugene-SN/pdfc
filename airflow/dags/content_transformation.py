# DAG 2: Content Transformation (–ò–°–ü–†–ê–í–õ–ï–ù–û)
# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π Markdown
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ vLLM —Å Qwen2.5-VL-32B-Instruct

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import requests
import json
import re
import os
from typing import Dict, List, Any, Optional

# –ò–º–ø–æ—Ä—Ç –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —É—Ç–∏–ª–∏—Ç
from shared_utils import (
    SharedUtils,
    NotificationUtils,
    ConfigUtils
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
DEFAULT_ARGS = {
    'owner': 'pdf-converter',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'content_transformation',
    default_args=DEFAULT_ARGS,
    description='DAG 2: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π Markdown',
    schedule_interval=None,  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ DAG 1
    max_active_runs=2,
    catchup=False,
    tags=['pdf-converter', 'content-transformation', 'markdown', 'vllm']
)

# =============================================================================
# vLLM CLIENT –î–õ–Ø CONTENT TRANSFORMATION
# =============================================================================
class VLLMContentTransformer:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ vLLM"""
    
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url
        self.model = model
        self.session = requests.Session()
    
    def get_transformation_prompt(self, content_type: str) -> str:
        """–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        base_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –∏–∑ PDF –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π Markdown.

–û–°–ù–û–í–ù–ê–Ø –ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞—Ç—å –∏–¥–µ–∞–ª—å–Ω—ã–π Markdown –¥–æ–∫—É–º–µ–Ω—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –°–æ—Ö—Ä–∞–Ω–∏ –í–°–Æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏, —Ç–∞–±–ª–∏—Ü—ã)
2. –°–æ–∑–¥–∞–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –∏–µ—Ä–∞—Ä—Ö–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (# ## ### ####)
3. –û—Ñ–æ—Ä–º–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã IPMI/BMC/Redfish –≤ –±–ª–æ–∫–∞—Ö –∫–æ–¥–∞ ```
4. –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ë–ï–ó –∏–∑–º–µ–Ω–µ–Ω–∏–π
5. –°–æ–∑–¥–∞–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ Markdown —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º
6. –ù–ï –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏—è –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ - —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π Markdown
7. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã

–§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï:
- –ó–∞–≥–æ–ª–æ–≤–∫–∏: –∏—Å–ø–æ–ª—å–∑—É–π # ## ### #### –ø–æ –∏–µ—Ä–∞—Ä—Ö–∏–∏
- –¢–∞–±–ª–∏—Ü—ã: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ |---|---|
- –ö–æ–¥/–∫–æ–º–∞–Ω–¥—ã: ```bash –∏–ª–∏ ```json –¥–ª—è –±–ª–æ–∫–æ–≤
- –°–ø–∏—Å–∫–∏: - –¥–ª—è –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, 1. –¥–ª—è –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö
- –í—ã–¥–µ–ª–µ–Ω–∏–µ: **–∂–∏—Ä–Ω—ã–π** –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–ï–†–ú–ò–ù–´ –°–û–•–†–ê–ù–Ø–¢–¨:
- IPMI –∫–æ–º–∞–Ω–¥—ã: chassis, power, mc, sensor, sel, sdr, fru
- BMC –∫–æ–º–∞–Ω–¥—ã: reset, info, watchdog, lan, user
- Redfish API: Systems, Chassis, Managers, UpdateService
- –ê–ø–ø–∞—Ä–∞—Ç–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏: CPU, RAM, SSD, NIC, GPU
- –ú–æ–¥–µ–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –∏ —Å–µ—Ä–∏–π–Ω—ã–µ –Ω–æ–º–µ—Ä–∞"""

        if content_type == "complex_table":
            return base_prompt + "\n\n–û–°–û–ë–û–ï –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–∂–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã. –°–æ–∑–¥–∞–π –∏–¥–µ–∞–ª—å–Ω—ã–µ Markdown —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç–æ–ª–±—Ü–æ–≤ –∏ —Å—Ç—Ä–æ–∫."
        elif content_type == "technical_specs":
            return base_prompt + "\n\n–û–°–û–ë–û–ï –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏. –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤."
        elif content_type == "commands":
            return base_prompt + "\n\n–û–°–û–ë–û–ï –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã. –û—Ñ–æ—Ä–º–∏ –∏—Ö –≤ –±–ª–æ–∫–∏ –∫–æ–¥–∞ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞."
        
        return base_prompt
    
    def transform_content_chunk(self, text_chunk: str, structure_info: Dict = None, content_type: str = "mixed") -> Optional[str]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ Markdown"""
        try:
            system_prompt = self.get_transformation_prompt(content_type)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            user_prompt = f"""–ü—Ä–µ–æ–±—Ä–∞–∑—É–π —ç—Ç–æ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∏–∑ PDF —Ç–µ–∫—Å—Ç –≤ –∏–¥–µ–∞–ª—å–Ω—ã–π Markdown:

–ò–°–•–û–î–ù–´–ô –ö–û–ù–¢–ï–ù–¢:
{text_chunk}"""

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
            if structure_info:
                user_prompt += f"""

–°–¢–†–£–ö–¢–£–†–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
{json.dumps(structure_info, ensure_ascii=False, indent=2)}"""
            
            # –ó–∞–ø—Ä–æ—Å –∫ vLLM
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 4096,
                "top_p": 0.9,
                "stream": False
            }
            
            response = self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                timeout=600
            )
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                markdown_content = result["choices"][0]["message"]["content"]
                return self.postprocess_markdown(markdown_content)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            return None
    
    def postprocess_markdown(self, markdown: str) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ Markdown"""
        if not markdown:
            return ""
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        cleaned = markdown.strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –≤–≤–æ–¥–Ω—ã—Ö —Ñ—Ä–∞–∑
        intro_patterns = [
            r'^[–í–≤]–æ—Ç Markdown –≤–µ—Ä—Å–∏—è[^:]*:?\s*',
            r'^[–í–≤]–æ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç[^:]*:?\s*',
            r'^[Mm]arkdown –≤–µ—Ä—Å–∏—è[^:]*:?\s*',
            r'^[Hh]ere is[^:]*:?\s*'
        ]
        
        for pattern in intro_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.MULTILINE)
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (—É–±–µ–¥–∏–º—Å—è —á—Ç–æ –µ—Å—Ç—å –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ #)
        cleaned = re.sub(r'^(#{1,6})([^\s#])', r'\1 \2', cleaned, flags=re.MULTILINE)
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
        lines = cleaned.split('\n')
        corrected_lines = []
        in_table = False
        
        for i, line in enumerate(lines):
            if '|' in line and line.count('|') >= 2:
                if not in_table:
                    # –ù–∞—á–∞–ª–æ —Ç–∞–±–ª–∏—Ü—ã - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫—É
                    if i + 1 < len(lines) and not re.match(r'^\|[\s\-\|:]+\|', lines[i + 1]):
                        corrected_lines.append(line)
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                        cols = line.count('|') - 1
                        separator = '|' + '---|' * cols
                        corrected_lines.append(separator)
                        in_table = True
                        continue
                    in_table = True
                corrected_lines.append(line)
            else:
                if in_table and line.strip() == '':
                    in_table = False
                corrected_lines.append(line)
        
        cleaned = '\n'.join(corrected_lines)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        
        return cleaned.strip()

# =============================================================================
# –§–£–ù–ö–¶–ò–ò DAG
# =============================================================================

def load_extraction_data(**context):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç DAG 1"""
    dag_run_conf = context['dag_run'].conf
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –æ—Ç DAG 1
    intermediate_file = dag_run_conf.get('intermediate_file')
    
    if not intermediate_file or not os.path.exists(intermediate_file):
        raise ValueError(f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ñ–∞–π–ª –æ—Ç DAG 1 –Ω–µ –Ω–∞–π–¥–µ–Ω: {intermediate_file}")
    
    # –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    with open(intermediate_file, 'r', encoding='utf-8') as f:
        extraction_data = json.load(f)
    
    if not extraction_data or 'extracted_content' not in extraction_data:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    transformation_session = {
        'session_id': f"transform_{int(datetime.now().timestamp())}",
        'extraction_data': extraction_data['extracted_content'],
        'analysis_data': extraction_data.get('analysis', {}),
        'source_file': extraction_data.get('source_file'),
        'original_config': dag_run_conf.get('original_config', {}),
        'vllm_model': 'Qwen/Qwen2.5-VL-32B-Instruct',  # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨
        'transformation_start_time': datetime.now().isoformat()
    }
    
    extracted_text = transformation_session['extraction_data'].get('extracted_text', '')
    tables_count = len(transformation_session['extraction_data'].get('tables', []))
    images_count = len(transformation_session['extraction_data'].get('images', []))
    
    print(f"üìÑ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:")
    print(f"   –¢–µ–∫—Å—Ç: {len(extracted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –¢–∞–±–ª–∏—Ü—ã: {tables_count}")
    print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {images_count}")
    
    return transformation_session

def analyze_content_structure(**context):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    transformation_session = context['task_instance'].xcom_pull(task_ids='load_extraction_data')
    
    extraction_data = transformation_session['extraction_data']
    extracted_text = extraction_data.get('extracted_text', '')
    document_structure = extraction_data.get('document_structure', {})
    tables = extraction_data.get('tables', [])
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_analysis = {
        'total_length': len(extracted_text),
        'has_structure': bool(document_structure),
        'tables_count': len(tables),
        'images_count': len(extraction_data.get('images', [])),
        'content_types': []
    }
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏
    text_blocks = split_into_blocks(extracted_text)
    
    analyzed_blocks = []
    for i, block in enumerate(text_blocks):
        block_type = classify_content_block(block)
        block_info = {
            'block_id': i,
            'content': block,
            'type': block_type,
            'length': len(block),
            'complexity': calculate_complexity(block)
        }
        analyzed_blocks.append(block_info)
        content_analysis['content_types'].append(block_type)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –±–ª–æ–∫–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    processing_groups = group_blocks_for_processing(analyzed_blocks)
    
    structure_analysis = {
        'content_analysis': content_analysis,
        'analyzed_blocks': analyzed_blocks,
        'processing_groups': processing_groups,
        'recommended_approach': determine_processing_approach(content_analysis)
    }
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–≤–µ—Ä—à–µ–Ω:")
    print(f"   –ë–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(analyzed_blocks)}")
    print(f"   –ì—Ä—É–ø–ø –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(processing_groups)}")
    print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥: {structure_analysis['recommended_approach']}")
    
    return structure_analysis

def split_into_blocks(text: str) -> List[str]:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏"""
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
    blocks = []
    current_block = []
    
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        
        # –ù–æ–≤—ã–π –±–ª–æ–∫ –ø—Ä–∏ –≤—Å—Ç—Ä–µ—á–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ –ø–æ—Å–ª–µ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏
        if (line.startswith('#') or 
            (not line and current_block) or
            len('\n'.join(current_block)) > 1000):
            
            if current_block:
                blocks.append('\n'.join(current_block))
                current_block = []
        
        if line:
            current_block.append(line)
    
    if current_block:
        blocks.append('\n'.join(current_block))
    
    return [block for block in blocks if block.strip()]

def classify_content_block(block: str) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –±–ª–æ–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    if not block.strip():
        return 'empty'
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    if any(line.strip().isupper() and len(line.strip()) < 100 for line in block.split('\n')[:2]):
        return 'header'
    
    # –¢–∞–±–ª–∏—Ü—ã (–ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º)
    table_indicators = ['|', '–∫–æ–ª–æ–Ω–∫–∞', '—Å—Ç–æ–ª–±–µ—Ü', '—Å—Ç—Ä–æ–∫–∞', '—Ç–∞–±–ª–∏—Ü–∞']
    if any(indicator in block.lower() for indicator in table_indicators):
        return 'table'
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã
    tech_indicators = ['ipmitool', 'redfish', 'bmc', '0x', 'chassis', 'power']
    if any(indicator in block.lower() for indicator in tech_indicators):
        return 'commands'
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
    if re.search(r'\d+\s*(GB|MHz|GHz|W|TB|MB|Gbps|RPM)', block):
        return 'technical_specs'
    
    # –°–ø–∏—Å–∫–∏
    list_patterns = [r'^\s*[-‚Ä¢]\s+', r'^\s*\d+[\.\)]\s+', r'^\s*[a-zA-Z]\)']
    if any(re.search(pattern, block, re.MULTILINE) for pattern in list_patterns):
        return 'list'
    
    # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
    return 'text'

def calculate_complexity(block: str) -> str:
    """–†–∞—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –±–ª–æ–∫–∞"""
    length = len(block)
    
    if length > 1000:
        return 'high'
    elif length > 300:
        return 'medium'
    else:
        return 'low'

def group_blocks_for_processing(blocks: List[Dict]) -> List[List[Dict]]:
    """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –±–ª–æ–∫–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    groups = []
    current_group = []
    current_group_length = 0
    max_group_length = 2000
    
    for block in blocks:
        block_length = block['length']
        
        # –°–ª–æ–∂–Ω—ã–µ –±–ª–æ–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
        if block['complexity'] == 'high' or block['type'] in ['table', 'commands']:
            if current_group:
                groups.append(current_group)
                current_group = []
                current_group_length = 0
            
            groups.append([block])
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ª–∏–º–∏—Ç–∞ –≥—Ä—É–ø–ø—ã
        if current_group_length + block_length > max_group_length and current_group:
            groups.append(current_group)
            current_group = [block]
            current_group_length = block_length
        else:
            current_group.append(block)
            current_group_length += block_length
    
    if current_group:
        groups.append(current_group)
    
    return groups

def determine_processing_approach(analysis: Dict) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
    if analysis['tables_count'] > 5:
        return 'table_focused'
    elif 'commands' in analysis['content_types']:
        return 'technical_focused'
    elif analysis['total_length'] > 10000:
        return 'chunked_processing'
    else:
        return 'standard'

def transform_content_blocks(**context):
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ Markdown"""
    transformation_session = context['task_instance'].xcom_pull(task_ids='load_extraction_data')
    structure_analysis = context['task_instance'].xcom_pull(task_ids='analyze_content_structure')
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vLLM —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
    transformer = VLLMContentTransformer(
        base_url=ConfigUtils.get_service_config()['vllm'],
        model=transformation_session['vllm_model']
    )
    
    processing_groups = structure_analysis['processing_groups']
    extraction_data = transformation_session['extraction_data']
    
    print(f"üîÑ –ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ {len(processing_groups)} –≥—Ä—É–ø–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    transformed_blocks = []
    processing_stats = {
        'groups_processed': 0,
        'blocks_processed': 0,
        'transformation_errors': 0
    }
    
    for group_idx, group in enumerate(processing_groups):
        print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã {group_idx + 1}/{len(processing_groups)}")
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ –≥—Ä—É–ø–ø—ã
        group_content = '\n\n'.join([block['content'] for block in group])
        group_types = [block['type'] for block in group]
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –≥—Ä—É–ø–ø—ã
        if 'table' in group_types:
            content_type = 'complex_table'
        elif 'commands' in group_types:
            content_type = 'commands'
        elif 'technical_specs' in group_types:
            content_type = 'technical_specs'
        else:
            content_type = 'mixed'
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –≥—Ä—É–ø–ø—ã
        transformed_content = transformer.transform_content_chunk(
            text_chunk=group_content,
            structure_info=extraction_data.get('document_structure', {}),
            content_type=content_type
        )
        
        if transformed_content:
            transformed_blocks.append(transformed_content)
            processing_stats['groups_processed'] += 1
            processing_stats['blocks_processed'] += len(group)
        else:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –≥—Ä—É–ø–ø—É {group_idx + 1}")
            processing_stats['transformation_errors'] += 1
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∫ fallback
            transformed_blocks.append(f"```\n{group_content}\n```")
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤
    final_markdown = '\n\n'.join(transformed_blocks)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –µ—Å–ª–∏ –µ—Å—Ç—å
    images = extraction_data.get('images', [])
    if images:
        final_markdown += "\n\n## –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n\n"
        for i, image_path in enumerate(images):
            final_markdown += f"![Image {i+1}]({image_path})\n\n"
    
    transformation_results = {
        'markdown_content': final_markdown,
        'processing_stats': processing_stats,
        'content_length': len(final_markdown),
        'transformation_quality': calculate_transformation_quality(final_markdown, extraction_data)
    }
    
    print(f"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    print(f"   Markdown: {len(final_markdown)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –ö–∞—á–µ—Å—Ç–≤–æ: {transformation_results['transformation_quality']}%")
    
    return transformation_results

def calculate_transformation_quality(markdown: str, extraction_data: Dict) -> float:
    """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    quality_score = 100.0
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    headers = len(re.findall(r'^#{1,6}\s+', markdown, re.MULTILINE))
    if headers == 0:
        quality_score -= 20
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü
    original_tables = len(extraction_data.get('tables', []))
    markdown_tables = len(re.findall(r'^\|.*\|', markdown, re.MULTILINE))
    if original_tables > 0 and markdown_tables < original_tables * 0.8:
        quality_score -= 15
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    if not re.search(r'^#\s+', markdown, re.MULTILINE):  # –ù–µ—Ç –≥–ª–∞–≤–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        quality_score -= 10
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤
    code_blocks = len(re.findall(r'```', markdown)) // 2
    if 'ipmitool' in extraction_data.get('extracted_text', '').lower() and code_blocks == 0:
        quality_score -= 10
    
    return max(0, quality_score)

def save_markdown_result(**context):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    transformation_session = context['task_instance'].xcom_pull(task_ids='load_extraction_data')
    transformation_results = context['task_instance'].xcom_pull(task_ids='transform_content_blocks')
    
    original_config = transformation_session['original_config']
    timestamp = original_config.get('timestamp', int(datetime.now().timestamp()))
    filename = original_config.get('filename', 'unknown.pdf')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø–∞–ø–∫—É –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–∏—Å—Ö–æ–¥–Ω—ã–π Markdown)
    output_path = SharedUtils.prepare_output_path(filename, 'zh', timestamp)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Markdown —Ñ–∞–π–ª–∞
    SharedUtils.save_final_result(
        content=transformation_results['markdown_content'],
        output_path=output_path,
        metadata={
            'source_file': transformation_session['source_file'],
            'transformation_model': transformation_session['vllm_model'],
            'processing_stats': transformation_results['processing_stats'],
            'transformation_quality': transformation_results['transformation_quality'],
            'content_length': transformation_results['content_length'],
            'processing_time': (datetime.now() - datetime.fromisoformat(transformation_session['transformation_start_time'])).total_seconds()
        }
    )
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG
    dag3_config = {
        'markdown_file': output_path,
        'markdown_content': transformation_results['markdown_content'],
        'original_config': original_config,
        'dag2_completed': True,
        'transformation_quality': transformation_results['transformation_quality']
    }
    
    print(f"üíæ Markdown —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    return dag3_config

# =============================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ê–î–ê–ß DAG
# =============================================================================

# –ó–∞–¥–∞—á–∞ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
load_data = PythonOperator(
    task_id='load_extraction_data',
    python_callable=load_extraction_data,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 2: –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
analyze_structure = PythonOperator(
    task_id='analyze_content_structure',
    python_callable=analyze_content_structure,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 3: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
transform_blocks = PythonOperator(
    task_id='transform_content_blocks',
    python_callable=transform_content_blocks,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
save_result = PythonOperator(
    task_id='save_markdown_result',
    python_callable=save_markdown_result,
    dag=dag
)

def notify_completion(**context):
    """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    dag3_config = context['task_instance'].xcom_pull(task_ids='save_markdown_result')
    transformation_session = context['task_instance'].xcom_pull(task_ids='load_extraction_data')
    
    quality = dag3_config['transformation_quality']
    
    message = f"""
    ‚úÖ DAG 2 (Content Transformation) —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω
    
    –ú–æ–¥–µ–ª—å: {transformation_session['vllm_model']}
    –ö–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: {quality}%
    Markdown —Ñ–∞–π–ª: {dag3_config['markdown_file']}
    
    –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø: Translation Pipeline (DAG 3)
    """
    
    print(message)
    NotificationUtils.send_success_notification(context, dag3_config)

# –ó–∞–¥–∞—á–∞ 5: –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
notify_task = PythonOperator(
    task_id='notify_completion',
    python_callable=notify_completion,
    dag=dag
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∑–∞–¥–∞—á
load_data >> analyze_structure >> transform_blocks >> save_result >> notify_task

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
def handle_failure(context):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤ DAG"""
    NotificationUtils.send_failure_notification(context, context.get('exception'))

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫ –∫–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º
for task in dag.tasks:
    task.on_failure_callback = handle_failure