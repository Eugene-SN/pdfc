# DAG 3: Translation Pipeline (–ò–°–ü–†–ê–í–õ–ï–ù–û)
# –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ ollama-translator
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å vLLM API –≤–º–µ—Å—Ç–æ Ollama

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import requests
import json
import re
import time
import hashlib
from typing import Dict, List, Any, Optional

# –ò–º–ø–æ—Ä—Ç –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —É—Ç–∏–ª–∏—Ç
from shared_utils import (
    SharedUtils,
    NotificationUtils,
    ConfigUtils
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è DAG
DEFAULT_ARGS = {
    'owner': 'pdf-converter',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'translation_pipeline',
    default_args=DEFAULT_ARGS,
    description='DAG 3: –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã',
    schedule_interval=None,  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ DAG 2
    max_active_runs=2,
    catchup=False,
    tags=['pdf-converter', 'translation', 'vllm', 'technical-terms']
)

# =============================================================================
# –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–ï–†–ú–ò–ù–´ (–ù–ï –ü–ï–†–ï–í–û–î–ò–¢–¨!)
# =============================================================================
TECHNICAL_TERMINOLOGY = {
    # Brand names (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!)
    "ÈóÆÂ§©": "WenTian",
    "ËÅîÊÉ≥ÈóÆÂ§©": "Lenovo WenTian",
    "Â§©Êìé": "ThinkSystem",
    "AnyBay": "AnyBay",
    
    # Processors
    "Ëá≥Âº∫": "Xeon",
    "ÂèØÊâ©Â±ïÂ§ÑÁêÜÂô®": "Scalable Processors",
    "Ëã±ÁâπÂ∞î": "Intel",
    "Â§ÑÁêÜÂô®": "Processor",
    "ÂÜÖÊ†∏": "Core",
    "Á∫øÁ®ã": "Thread",
    "ÁùøÈ¢ë": "Turbo Boost",
    
    # Memory and storage
    "ÂÜÖÂ≠ò": "Memory",
    "Â≠òÂÇ®": "Storage",
    "Á°¨Áõò": "Drive",
    "Âõ∫ÊÄÅÁ°¨Áõò": "SSD",
    "Êú∫Ê¢∞Á°¨Áõò": "HDD",
    "ÁÉ≠ÊèíÊãî": "Hot-swap",
    "ÂÜó‰Ωô": "Redundancy",
    "ËÉåÊùø": "Backplane",
    "ÊâòÊû∂": "Tray",
    
    # Network
    "‰ª•Â§™ÁΩë": "Ethernet",
    "ÂÖâÁ∫§": "Fiber",
    "Â∏¶ÂÆΩ": "Bandwidth",
    "Âª∂Ëøü": "Latency",
    "ÁΩëÂç°": "Network Adapter",
    
    # Form factors
    "Ëã±ÂØ∏": "inch",
    "Êú∫Êû∂": "Rack",
    "ÊèíÊßΩ": "Slot",
    "ËΩ¨Êé•Âç°": "Riser Card",
    
    # Power
    "ÁîµÊ∫ê": "Power Supply",
    "ÈìÇÈáë": "Platinum",
    "ÈíõÈáë": "Titanium",
    "CRPS": "CRPS"
}

TECHNICAL_TERMINOLOGY_RU = {
    "ÈóÆÂ§©": "WenTian",
    "ËÅîÊÉ≥ÈóÆÂ§©": "Lenovo WenTian",
    "Ëá≥Âº∫": "Xeon",
    "ÂèØÊâ©Â±ïÂ§ÑÁêÜÂô®": "Scalable –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã",
    "Ëã±ÁâπÂ∞î": "Intel",
    "Â§ÑÁêÜÂô®": "–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä",
    "ÂÜÖÊ†∏": "—è–¥—Ä–æ",
    "Á∫øÁ®ã": "–ø–æ—Ç–æ–∫",
    "ÂÜÖÂ≠ò": "–ø–∞–º—è—Ç—å",
    "Â≠òÂÇ®": "—Ö—Ä–∞–Ω–∏–ª–∏—â–µ",
    "Á°¨Áõò": "–¥–∏—Å–∫",
    "Âõ∫ÊÄÅÁ°¨Áõò": "SSD",
    "Êú∫Ê¢∞Á°¨Áõò": "HDD",
    "ÁÉ≠ÊèíÊãî": "–≥–æ—Ä—è—á–∞—è –∑–∞–º–µ–Ω–∞",
    "ÂÜó‰Ωô": "—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∏–µ",
    "ÊâòÊû∂": "–ª–æ—Ç–æ–∫",
    "‰ª•Â§™ÁΩë": "Ethernet",
    "Â∏¶ÂÆΩ": "–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å",
    "Âª∂Ëøü": "–∑–∞–¥–µ—Ä–∂–∫–∞",
    "Êú∫Êû∂": "—Å—Ç–æ–π–∫–∞",
    "ÊèíÊßΩ": "—Å–ª–æ—Ç",
    "ÁîµÊ∫ê": "–±–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è"
}

# =============================================================================
# vLLM API –ö–õ–ò–ï–ù–¢ (–ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–´–ô –ò–ó OLLAMA-TRANSLATOR)
# =============================================================================
class VLLMTranslationClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å vLLM API –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
    
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url
        self.model = model
        self.session = requests.Session()
        self.translation_cache = {}
        
    def get_system_prompt(self, source_lang: str, target_lang: str) -> str:
        """–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ–º thinking —Ä–µ–∂–∏–º–∞"""
        return f"""–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–µ—Ä–µ–≤–æ–¥—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–µ—Ä–≤–µ—Ä–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.

–û–°–ù–û–í–ù–ê–Ø –ó–ê–î–ê–ß–ê: –ü–µ—Ä–µ–≤–æ–¥–∏—Ç–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã —Å {source_lang} –Ω–∞ {target_lang} —è–∑—ã–∫.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –¢–û–õ–¨–ö–û –≥–æ—Ç–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥, –ë–ï–ó –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
2. –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—Ä–∞–∑—ã "–í–æ—Ç –ø–µ—Ä–µ–≤–æ–¥", "Here is translation"
3. –ù–ï –¥–æ–±–∞–≤–ª—è–π—Ç–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –≤ —Ç–µ–≥–∞—Ö <–¥—É–º–∞—é>, –∏–ª–∏ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö
4. –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ –í–°–ï —á–∏—Å–ª–∞, –∫–æ–¥—ã, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¢–û–ß–ù–û
5. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ thinking —Ä–µ–∂–∏–º - –æ—Ç–≤–µ—á–∞–π—Ç–µ —Å—Ä–∞–∑—É –∏ –∫—Ä–∞—Ç–∫–æ

–ë–†–ï–ù–î–´ (–ù–ï –ú–ï–ù–Ø–¢–¨!):
- "ÈóÆÂ§©" –í–°–ï–ì–î–ê ‚Üí "WenTian"
- "ËÅîÊÉ≥ÈóÆÂ§©" –í–°–ï–ì–î–ê ‚Üí "Lenovo WenTian"
- –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ: Intel, Xeon, PCIe, DDR5, NVMe, SAS, SATA

–ö–û–ú–ê–ù–î–´ –ù–ï –ü–ï–†–ï–í–û–î–ò–¢–¨:
- IPMI: chassis, power, mc, sensor, sel, sdr, fru
- API: get, set, list, status, activate, deactivate
- Hex –∫–æ–¥—ã: 0x30, 0x02
- –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã: GPU, CPU, SSD, HDD, RAID, BMC

–§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï:
- –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É Markdown —Ç–∞–±–ª–∏—Ü –¢–û–ß–ù–û
- –ù–ï –∏–∑–º–µ–Ω—è–π—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤
- –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ HTML —Ç–µ–≥–∏ –∏ –æ—Ç—Å—Ç—É–ø—ã

–í—ã–≤–æ–¥–∏—Ç–µ –¢–û–õ–¨–ö–û {target_lang} –ø–µ—Ä–µ–≤–æ–¥ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞!"""

    def get_user_prompt(self, text: str, source_lang: str, target_lang: str) -> str:
        """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç"""
        return f"""–ü–µ—Ä–µ–≤–µ–¥–∏—Ç–µ —ç—Ç–æ—Ç {source_lang} —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ {target_lang} —è–∑—ã–∫:

{text}"""

    def make_translation_request(self, text: str, source_lang: str, target_lang: str) -> Optional[str]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫ vLLM"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            cache_key = self.get_cache_key(text, source_lang, target_lang)
            if cache_key in self.translation_cache:
                print(f"üì¶ –ü–æ–ª—É—á–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –∫—ç—à–∞")
                return self.translation_cache[cache_key]

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
            system_prompt = self.get_system_prompt(source_lang, target_lang)
            user_prompt = self.get_user_prompt(text, source_lang, target_lang)
            
            # –ó–∞–ø—Ä–æ—Å –∫ vLLM API
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 4096,
                "top_p": 0.9,
                "stream": False
            }
            
            response = self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                timeout=600
            )
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and len(result["choices"]) > 0:
                translated = result["choices"][0]["message"]["content"]
                
                # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
                cleaned = self.postprocess_translation(translated, target_lang)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
                self.translation_cache[cache_key] = cleaned
                
                return cleaned
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return None
    
    def postprocess_translation(self, response: str, target_lang: str) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –æ—á–∏—Å—Ç–∫–æ–π thinking —Ä–µ–∂–∏–º–∞"""
        if not response:
            return ""
        
        cleaned = response.strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ thinking —Ç–µ–≥–æ–≤ –∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
        thinking_patterns = [
            r'<–¥—É–º–∞—é>.*?</–¥—É–º–∞—é>',
            r'<thinking>.*?</thinking>',
            r'[–•—Ö]–æ—Ä–æ—à–æ[,\s]*–º–Ω–µ –Ω—É–∂–Ω–æ[^.]*?\.',
            r'[–°—Å]–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä[^.]*?\.',
            r'Let me[^.]*?\.',
            r'First[,\s]*I[^.]*?\.',
            r'[–í–≤]–æ—Ç –ø–µ—Ä–µ–≤–æ–¥[^:]*:?\s*',
            r'Here is[^:]*:?\s*',
            r'[–ù–Ω]–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω[^:]*:?\s*'
        ]
        
        for pattern in thinking_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.DOTALL)
        
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        cleaned = self.fix_technical_terms(cleaned, target_lang)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    def fix_technical_terms(self, text: str, target_lang: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ: –ù–ï –ú–ï–ù–Ø–¢–¨ WenTian –Ω–∞ ThinkSystem!
        brand_fixes = {
            r'\b[Qq]itian\b': 'WenTian',
            r'\bSkyland\b': 'WenTian',
            r'\bSkyStorage\b': 'WenTian'
        }
        
        for wrong_pattern, correct in brand_fixes.items():
            if re.search(wrong_pattern, text):
                text = re.sub(wrong_pattern, correct, text)
        
        # –í—ã–±–æ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if target_lang == "ru":
            terminology = TECHNICAL_TERMINOLOGY_RU
        else:
            terminology = TECHNICAL_TERMINOLOGY
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        for chinese_term, translation in terminology.items():
            if chinese_term in text:
                text = text.replace(chinese_term, translation)
        
        return text
    
    def get_cache_key(self, text: str, source_lang: str, target_lang: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        content = f"{source_lang}-{target_lang}-{text}"
        return hashlib.md5(content.encode()).hexdigest()[:16]

# =============================================================================
# –§–£–ù–ö–¶–ò–ò DAG
# =============================================================================

def initialize_translation(**context):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    dag_run_conf = context['dag_run'].conf
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ DAG
    markdown_file = dag_run_conf.get('markdown_file')
    original_config = dag_run_conf.get('original_config', {})
    
    target_language = original_config.get('target_language', 'ru')
    
    # –ß—Ç–µ–Ω–∏–µ Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    if markdown_file and SharedUtils.validate_input_file(markdown_file.replace('.md', '.pdf')):
        try:
            with open(markdown_file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()
        except:
            # Fallback: —á–∏—Ç–∞–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            markdown_content = dag_run_conf.get('markdown_content', '')
    else:
        markdown_content = dag_run_conf.get('markdown_content', '')
    
    if not markdown_content:
        raise ValueError("–ù–µ—Ç Markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–∞
    translation_session = {
        'session_id': f"translation_{int(datetime.now().timestamp())}",
        'source_language': 'zh-CN',
        'target_language': target_language,
        'markdown_content': markdown_content,
        'original_config': original_config,
        'translation_model': 'Qwen/Qwen3-30B-A3B-Instruct-2507',  # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨
        'preserve_technical_terms': True,
        'batch_processing': True,
        'lines_total': len(markdown_content.split('\n')),
        'processing_start_time': datetime.now().isoformat()
    }
    
    print(f"üåê –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞: {target_language}")
    print(f"üìä –°—Ç—Ä–æ–∫ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: {translation_session['lines_total']}")
    
    return translation_session

def batch_translate_content(**context):
    """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    translation_session = context['task_instance'].xcom_pull(task_ids='initialize_translation')
    
    markdown_content = translation_session['markdown_content']
    target_language = translation_session['target_language']
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vLLM –∫–ª–∏–µ–Ω—Ç–∞
    vllm_client = VLLMTranslationClient(
        base_url=ConfigUtils.get_service_config()['vllm'],
        model=translation_session['translation_model']
    )
    
    print(f"üöÄ –ù–∞—á–∞–ª–æ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ {target_language}")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞
    lines = markdown_content.split('\n')
    
    # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    batches = create_smart_batches(lines)
    
    translated_lines = []
    batch_count = 0
    
    for batch in batches:
        batch_count += 1
        batch_content = '\n'.join(batch)
        
        if not batch_content.strip():
            translated_lines.extend(batch)
            continue
        
        print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_count}/{len(batches)}")
        
        # –ü–µ—Ä–µ–≤–æ–¥ –±–∞—Ç—á–∞
        translated_batch = vllm_client.make_translation_request(
            batch_content, '–∫–∏—Ç–∞–π—Å–∫–∏–π', get_language_name(target_language)
        )
        
        if translated_batch:
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ —Å—Ç—Ä–æ–∫–∏
            batch_lines = translated_batch.split('\n')
            if len(batch_lines) == len(batch):
                translated_lines.extend(batch_lines)
            else:
                # –ï—Å–ª–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, –ø–µ—Ä–µ–≤–æ–¥–∏–º –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
                for line in batch:
                    if line.strip():
                        translated_line = vllm_client.make_translation_request(
                            line, '–∫–∏—Ç–∞–π—Å–∫–∏–π', get_language_name(target_language)
                        )
                        translated_lines.append(translated_line or line)
                    else:
                        translated_lines.append(line)
        else:
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –±–∞—Ç—á–∞ –Ω–µ —É–¥–∞–ª—Å—è, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
            translated_lines.extend(batch)
        
        # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
        time.sleep(0.2)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    translated_content = '\n'.join(translated_lines)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞
    quality_score = validate_translation_quality(markdown_content, translated_content, target_language)
    
    translation_results = {
        'translated_content': translated_content,
        'source_lines': len(lines),
        'batches_processed': len(batches),
        'quality_score': quality_score,
        'translation_stats': {
            'input_length': len(markdown_content),
            'output_length': len(translated_content),
            'chinese_remaining': len(re.findall(r'[\u4e00-\u9fff]', translated_content))
        }
    }
    
    print(f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score}%")
    return translation_results

def create_smart_batches(lines: List[str]) -> List[List[str]]:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π –ø–æ —Ç–∏–ø—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    batches = []
    current_batch = []
    max_batch_size = 6
    
    for line in lines:
        content_type = analyze_line_complexity(line)
        optimal_batch_size = get_optimal_batch_size(content_type)
        
        if len(current_batch) >= optimal_batch_size and current_batch:
            batches.append(current_batch)
            current_batch = [line]
        else:
            current_batch.append(line)
    
    if current_batch:
        batches.append(current_batch)
    
    return batches

def analyze_line_complexity(line: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∏–Ω–≥–∞"""
    if not line.strip():
        return 'empty'
    
    if line.strip().startswith('#'):
        return 'header'
    
    if '|' in line and line.count('|') > 2:
        return 'table'
    
    if any(re.search(pattern, line) for pattern in [r'\d+\s*(GB|MB|GHz|MHz|W|TB)', r'ipmitool', r'0x[0-9a-f]+']):
        return 'technical_specs'
    
    if len(line) > 100:
        return 'long_text'
    
    return 'text'

def get_optimal_batch_size(content_type: str) -> int:
    """–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    batch_sizes = {
        'empty': 20,
        'header': 4,
        'table': 2,
        'technical_specs': 3,
        'long_text': 1,
        'text': 6
    }
    return batch_sizes.get(content_type, 4)

def get_language_name(lang_code: str) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∏–º–µ–Ω–∏ —è–∑—ã–∫–∞"""
    languages = {
        'ru': '—Ä—É—Å—Å–∫–∏–π',
        'en': '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π',
        'zh': '–∫–∏—Ç–∞–π—Å–∫–∏–π'
    }
    return languages.get(lang_code, lang_code)

def validate_translation_quality(original: str, translated: str, target_lang: str) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ ollama-translator)"""
    quality_score = 100.0
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    chinese_count = len(re.findall(r'[\u4e00-\u9fff]', translated))
    if chinese_count > 0:
        quality_score -= min(50, chinese_count * 3)
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –∏ thinking
    thinking_patterns = [
        r'[–•—Ö]–æ—Ä–æ—à–æ[,\s]*–º–Ω–µ', r'[–°—Å]–Ω–∞—á–∞–ª–∞ –ø–æ—Å–º–æ—Ç—Ä', r'Let me', r'First I',
        r'[–í–≤]–æ—Ç –ø–µ—Ä–µ–≤–æ–¥', r'Here is', r'[–ù–Ω]–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω',
        r'<–¥—É–º–∞—é>', r'</–¥—É–º–∞—é>', r'<thinking>', r'</thinking>'
    ]
    
    thinking_count = sum(len(re.findall(pattern, translated)) for pattern in thinking_patterns)
    if thinking_count > 0:
        quality_score -= min(30, thinking_count * 10)
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –±—Ä–µ–Ω–¥–æ–≤
    if "ÈóÆÂ§©" in original and "WenTian" not in translated:
        quality_score -= 20
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü
    orig_table_rows = len(re.findall(r'^\|.*\|$', original, re.MULTILINE))
    trans_table_rows = len(re.findall(r'^\|.*\|$', translated, re.MULTILINE))
    
    if orig_table_rows > 0:
        table_preservation = trans_table_rows / orig_table_rows
        if table_preservation < 0.9:
            quality_score -= 15
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
    size_ratio = len(translated) / max(len(original), 1)
    if size_ratio > 2.5 or size_ratio < 0.4:
        quality_score -= 10
    
    return max(0, quality_score)

def intelligent_fix_remaining(**context):
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    translation_results = context['task_instance'].xcom_pull(task_ids='batch_translate_content')
    translation_session = context['task_instance'].xcom_pull(task_ids='initialize_translation')
    
    translated_content = translation_results['translated_content']
    chinese_remaining = translation_results['translation_stats']['chinese_remaining']
    
    if chinese_remaining == 0:
        print("‚úÖ –ö–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        return translation_results
    
    print(f"üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {chinese_remaining} –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∑–∞–ø—É—Å–∫–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    vllm_client = VLLMTranslationClient(
        base_url=ConfigUtils.get_service_config()['vllm'],
        model=translation_session['translation_model']
    )
    
    # –ü–æ–∏—Å–∫ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    chinese_fragments = []
    for match in re.finditer(r'[\u4e00-\u9fff]+', translated_content):
        fragment = match.group()
        if len(fragment) >= 2:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
            chinese_fragments.append((fragment, match.span()))
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ –æ–¥–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É
    current_text = translated_content
    fixes_count = 0
    target_language = translation_session['target_language']
    
    for fragment, span in chinese_fragments[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10 –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        translated_fragment = vllm_client.make_translation_request(
            fragment, '–∫–∏—Ç–∞–π—Å–∫–∏–π', get_language_name(target_language)
        )
        
        if translated_fragment and fragment != translated_fragment and not re.search(r'[\u4e00-\u9fff]', translated_fragment):
            current_text = current_text.replace(fragment, translated_fragment, 1)
            fixes_count += 1
            print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω #{fixes_count}: {fragment} ‚Üí {translated_fragment[:20]}...")
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_chinese_count = len(re.findall(r'[\u4e00-\u9fff]', current_text))
    improvement = chinese_remaining - final_chinese_count
    
    print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {chinese_remaining} ‚Üí {final_chinese_count} (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {improvement})")
    
    translation_results['translated_content'] = current_text
    translation_results['translation_stats']['chinese_remaining'] = final_chinese_count
    translation_results['fixes_applied'] = fixes_count
    
    return translation_results

def save_translation_result(**context):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    translation_session = context['task_instance'].xcom_pull(task_ids='initialize_translation')
    translation_results = context['task_instance'].xcom_pull(task_ids='intelligent_fix_remaining')
    
    original_config = translation_session['original_config']
    target_language = translation_session['target_language']
    timestamp = original_config.get('timestamp', int(datetime.now().timestamp()))
    filename = original_config.get('filename', 'unknown.pdf')
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    output_path = SharedUtils.prepare_output_path(filename, target_language, timestamp)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    SharedUtils.save_final_result(
        content=translation_results['translated_content'],
        output_path=output_path,
        metadata={
            'source_file': original_config.get('input_file'),
            'target_language': target_language,
            'translation_model': translation_session['translation_model'],
            'translation_stats': translation_results['translation_stats'],
            'quality_score': translation_results['quality_score'],
            'processing_time': (datetime.now() - datetime.fromisoformat(translation_session['processing_start_time'])).total_seconds(),
            'fixes_applied': translation_results.get('fixes_applied', 0)
        }
    )
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ DAG
    dag4_config = {
        'translated_file': output_path,
        'translated_content': translation_results['translated_content'],
        'translation_metadata': {
            'target_language': target_language,
            'quality_score': translation_results['quality_score'],
            'source_lines': translation_results['source_lines'],
            'translation_stats': translation_results['translation_stats']
        },
        'original_config': original_config,
        'dag3_completed': True
    }
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–≤–æ–¥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    return dag4_config

# =============================================================================
# –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ó–ê–î–ê–ß DAG
# =============================================================================

# –ó–∞–¥–∞—á–∞ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞
init_translation = PythonOperator(
    task_id='initialize_translation',
    python_callable=initialize_translation,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 2: –ü–∞–∫–µ—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
batch_translate = PythonOperator(
    task_id='batch_translate_content',
    python_callable=batch_translate_content,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 3: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤
fix_remaining = PythonOperator(
    task_id='intelligent_fix_remaining',
    python_callable=intelligent_fix_remaining,
    dag=dag
)

# –ó–∞–¥–∞—á–∞ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
save_result = PythonOperator(
    task_id='save_translation_result',
    python_callable=save_translation_result,
    dag=dag
)

def notify_completion(**context):
    """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    dag4_config = context['task_instance'].xcom_pull(task_ids='save_translation_result')
    translation_session = context['task_instance'].xcom_pull(task_ids='initialize_translation')
    
    target_language = translation_session['target_language']
    quality_score = dag4_config['translation_metadata']['quality_score']
    
    message = f"""
    ‚úÖ DAG 3 (Translation Pipeline) —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω
    
    –Ø–∑—ã–∫ –ø–µ—Ä–µ–≤–æ–¥–∞: {target_language}
    –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.1f}%
    –ú–æ–¥–µ–ª—å: {translation_session['translation_model']}
    –§–∞–π–ª: {dag4_config['translated_file']}
    
    –°–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø: Quality Assurance (DAG 4)
    """
    
    print(message)
    NotificationUtils.send_success_notification(context, dag4_config)

# –ó–∞–¥–∞—á–∞ 5: –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
notify_task = PythonOperator(
    task_id='notify_completion',
    python_callable=notify_completion,
    dag=dag
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∑–∞–¥–∞—á
init_translation >> batch_translate >> fix_remaining >> save_result >> notify_task

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
def handle_failure(context):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤ DAG"""
    NotificationUtils.send_failure_notification(context, context.get('exception'))

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫ –∫–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º
for task in dag.tasks:
    task.on_failure_callback = handle_failure