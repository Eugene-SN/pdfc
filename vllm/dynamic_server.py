#!/usr/bin/env python3
"""
–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π vLLM —Å–µ—Ä–≤–µ—Ä —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–º–µ–Ω–æ–π –º–æ–¥–µ–ª–µ–π
PDF Converter Pipeline v2.0 - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""

import asyncio
import uvicorn
import logging
import os
import time
from contextlib import asynccontextmanager
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from model_manager import model_manager, TaskType, initialize_model_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Pydantic –º–æ–¥–µ–ª–∏
class ChatMessage(BaseModel):
    role: str = Field(..., description="–†–æ–ª—å: system, user, assistant")
    content: str = Field(..., description="–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è")

class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    messages: List[ChatMessage] = Field(..., description="–°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π")
    temperature: float = Field(0.1, ge=0.0, le=2.0)
    max_tokens: int = Field(4096, ge=1, le=32768)
    top_p: float = Field(0.9, ge=0.0, le=1.0)
    top_k: int = Field(50, ge=1, le=100)
    stream: bool = Field(False)
    task_type: Optional[str] = Field(None, description="–¢–∏–ø –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏")

class ModelSwapRequest(BaseModel):
    model_key: str = Field(..., description="–ö–ª—é—á –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")

# Lifespan
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Dynamic vLLM Server –¥–ª—è PDF Converter Pipeline v2.0")
    
    success = await initialize_model_manager()
    if not success:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Model Manager")
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Model Manager")
    
    logger.info("‚úÖ Dynamic vLLM Server –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    yield
    
    # Shutdown
    logger.info("üîÑ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Dynamic vLLM Server")
    if model_manager.current_model:
        await model_manager.unload_current_model()
    logger.info("‚úÖ Dynamic vLLM Server –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# FastAPI app
app = FastAPI(
    title="Dynamic vLLM Server",
    description="vLLM —Å–µ—Ä–≤–µ—Ä —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π",
    version="2.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def determine_task_type_from_messages(messages: List[ChatMessage]) -> TaskType:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π"""
    try:
        combined_text = " ".join([msg.content.lower() for msg in messages])
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è Content Transformation
        content_keywords = [
            "–ø—Ä–µ–æ–±—Ä–∞–∑—É–π", "markdown", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", "—Ç–∞–±–ª–∏—Ü–∞", 
            "pdf", "–¥–æ–∫—É–º–µ–Ω—Ç", "–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ", "—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è Translation  
        translation_keywords = [
            "–ø–µ—Ä–µ–≤–µ–¥–∏", "translate", "–ø–µ—Ä–µ–≤–æ–¥", "translation",
            "—Ä—É—Å—Å–∫–∏–π", "english", "‰∏≠Êñá", "—è–∑—ã–∫", "language"
        ]
        
        content_score = sum(1 for keyword in content_keywords if keyword in combined_text)
        translation_score = sum(1 for keyword in translation_keywords if keyword in combined_text)
        
        if translation_score > content_score:
            return TaskType.TRANSLATION
        else:
            return TaskType.CONTENT_TRANSFORMATION
            
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏: {e}")
        return TaskType.CONTENT_TRANSFORMATION

@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π endpoint —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–º–µ–Ω–æ–π –º–æ–¥–µ–ª–µ–π"""
    try:
        start_time = time.time()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        task_type = None
        if request.task_type:
            try:
                task_type = TaskType(request.task_type)
            except ValueError:
                pass
                
        if not task_type:
            task_type = determine_task_type_from_messages(request.messages)
            
        logger.info(f"üìù –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏. –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type.value}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω—É–∂–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_ready = await model_manager.ensure_model_loaded(task_type)
        if not model_ready:
            raise HTTPException(
                status_code=503,
                detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏ {task_type.value}"
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ vLLM engine
        if not model_manager.vllm_engine:
            raise HTTPException(status_code=503, detail="vLLM engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è Qwen —Ñ–æ—Ä–º–∞—Ç–∞
        prompt_parts = []
        for message in request.messages:
            if message.role == "system":
                prompt_parts.append(f"<|im_start|>system\n{message.content}<|im_end|>")
            elif message.role == "user":
                prompt_parts.append(f"<|im_start|>user\n{message.content}<|im_end|>")
            elif message.role == "assistant":
                prompt_parts.append(f"<|im_start|>assistant\n{message.content}<|im_end|>")
                
        prompt_parts.append("<|im_start|>assistant\n")
        formatted_prompt = "\n".join(prompt_parts)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        from vllm import SamplingParams
        from vllm.utils import random_uuid
        
        sampling_params = SamplingParams(
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            top_p=request.top_p,
            top_k=request.top_k,
            stop=["<|im_end|>"],
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        request_id = random_uuid()
        results = model_manager.vllm_engine.generate(
            formatted_prompt,
            sampling_params,
            request_id=request_id
        )
        
        # –û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        final_output = None
        async for request_output in results:
            final_output = request_output
            
        if final_output is None or not final_output.outputs:
            raise HTTPException(status_code=500, detail="No output generated")
            
        generated_text = final_output.outputs[0].text.strip()
        
        # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
        prompt_tokens = len(final_output.prompt_token_ids) if final_output.prompt_token_ids else 0
        completion_tokens = len(final_output.outputs.token_ids) if final_output.outputs.token_ids else 0
        total_tokens = prompt_tokens + completion_tokens
        
        processing_time = time.time() - start_time
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        response = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion", 
            "created": int(time.time()),
            "model": model_manager.models[model_manager.current_model].name,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": generated_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens
            },
            "pdf_converter_meta": {
                "task_type": task_type.value,
                "model_key": model_manager.current_model,
                "processing_time_seconds": round(processing_time, 2),
                "vram_usage_gb": round(48.0 - model_manager.get_available_vram_gb(), 1)
            }
        }
        
        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {processing_time:.2f}s. –¢–æ–∫–µ–Ω–æ–≤: {total_tokens}")
        return response
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")

@app.post("/v1/models/swap")
async def swap_model(request: ModelSwapRequest):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏"""
    try:
        logger.info(f"üîÑ –ó–∞–ø—Ä–æ—Å —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ –Ω–∞: {request.model_key}")
        
        if request.model_key not in model_manager.models:
            available_models = list(model_manager.models.keys())
            raise HTTPException(
                status_code=400,
                detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {request.model_key}. –î–æ—Å—Ç—É–ø–Ω—ã: {available_models}"
            )
        
        success = await model_manager.load_model(request.model_key)
        if success:
            return {
                "status": "success",
                "message": f"–ú–æ–¥–µ–ª—å {request.model_key} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
                "current_model": model_manager.current_model,
                "vram_available": model_manager.get_available_vram_gb()
            }
        else:
            raise HTTPException(
                status_code=503,
                detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {request.model_key}"
            )
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/models")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    models_info = {}
    for model_key, config in model_manager.models.items():
        models_info[model_key] = {
            "name": config.name,
            "alias": config.alias,
            "task_type": config.task_type.value,
            "state": model_manager.model_states[model_key].value
        }
    
    models_list = []
    for model_key, info in models_info.items():
        models_list.append({
            "id": info["name"],
            "object": "model",
            "created": int(time.time()),
            "owned_by": "pdf-converter-v2",
            "pdf_converter_meta": {
                "key": model_key,
                "alias": info["alias"],
                "task_type": info["task_type"],
                "state": info["state"]
            }
        })
    
    return {"object": "list", "data": models_list}

@app.get("/v1/models/status")
async def models_status():
    """–î–µ—Ç–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã –º–æ–¥–µ–ª–µ–π"""
    return {
        "manager_status": model_manager.get_status(),
        "system_info": {
            "total_vram_gb": 96.0,  # 2x A6000
            "available_vram_gb": model_manager.get_available_vram_gb(),
            "gpu_count": 2,
            "gpu_type": "NVIDIA A6000"
        }
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞"""
    status = model_manager.get_status()
    is_healthy = (
        model_manager.current_model is not None and
        status["available_vram_gb"] > 2.0
    )
    
    response = {
        "status": "healthy" if is_healthy else "unhealthy",
        "timestamp": time.time(),
        "current_model": status["current_model"],
        "available_vram_gb": status["available_vram_gb"],
        "version": "v2.0-dynamic"
    }
    
    return JSONResponse(
        content=response,
        status_code=200 if is_healthy else 503
    )

@app.get("/metrics")
async def metrics():
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Prometheus"""
    status = model_manager.get_status()
    
    metrics_lines = [
        "# HELP vllm_available_vram_gb Available VRAM in GB",
        "# TYPE vllm_available_vram_gb gauge",
        f'vllm_available_vram_gb {status["available_vram_gb"]}',
        "",
        "# HELP vllm_model_loaded Current model loaded (1=loaded, 0=not loaded)",
        "# TYPE vllm_model_loaded gauge"
    ]
    
    for model_key, config in model_manager.models.items():
        is_loaded = 1 if model_manager.model_states[model_key].name == "loaded" else 0
        metrics_lines.append(f'vllm_model_loaded{{model_key="{model_key}",model_name="{config.name}"}} {is_loaded}')
    
    return Response(
        content="\n".join(metrics_lines),
        media_type="text/plain"
    )

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8000"))
    
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ Dynamic vLLM Server –Ω–∞ {host}:{port}")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True,
        workers=1,  # –í–∞–∂–Ω–æ: —Ç–æ–ª—å–∫–æ 1 worker –¥–ª—è GPU
        reload=False
    )
