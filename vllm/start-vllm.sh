#!/bin/bash
# vLLM Start Script –¥–ª—è PDF Converter Pipeline v2.0
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞

set -e

echo "üöÄ –ó–∞–ø—É—Å–∫ vLLM Server –¥–ª—è PDF Converter Pipeline v2.0"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_MODEL="${VLLM_MODEL_NAME:-Qwen/Qwen2.5-VL-32B-Instruct}"
MODEL_ALIAS="${VLLM_MODEL_ALIAS:-Qwen2.5-VL-32B-Instruct}"
SERVED_MODEL_NAME="${MODEL_ALIAS}"

# GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
TENSOR_PARALLEL_SIZE="${VLLM_TENSOR_PARALLEL_SIZE:-2}"
GPU_MEMORY_UTILIZATION="${VLLM_GPU_MEMORY_UTILIZATION:-0.9}"
MAX_MODEL_LEN="${VLLM_MAX_MODEL_LEN:-8192}"
MAX_NUM_SEQS="${VLLM_MAX_NUM_SEQS:-32}"
BLOCK_SIZE="${VLLM_BLOCK_SIZE:-16}"
SWAP_SPACE="${VLLM_SWAP_SPACE:-4}"

# Generation –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
TEMPERATURE="${VLLM_TEMPERATURE:-0.1}"
MAX_TOKENS="${VLLM_MAX_TOKENS:-4096}"
TOP_P="${VLLM_TOP_P:-0.9}"
TOP_K="${VLLM_TOP_K:-50}"

# –°–µ—Ä–≤–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
HOST="${HOST:-0.0.0.0}"
PORT="${PORT:-8000}"
API_KEY="${VLLM_API_KEY:-pdf-converter-secure-key-2024}"

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
DTYPE="${DTYPE:-auto}"
TRUST_REMOTE_CODE="${TRUST_REMOTE_CODE:-true}"
ENABLE_PREFIX_CACHING="${ENABLE_PREFIX_CACHING:-true}"

echo "üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è vLLM:"
echo "   –ú–æ–¥–µ–ª—å: $DEFAULT_MODEL"
echo "   –ê–ª–∏–∞—Å: $MODEL_ALIAS"
echo "   Tensor Parallel: $TENSOR_PARALLEL_SIZE"
echo "   GPU Memory: $GPU_MEMORY_UTILIZATION"
echo "   Max Model Length: $MAX_MODEL_LEN"
echo "   Host: $HOST:$PORT"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
echo "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU..."
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi
    echo "‚úÖ GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
else
    echo "‚ö†Ô∏è nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ GPU –ø—Ä–æ–≤–µ—Ä–∫–∏"
fi

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
mkdir -p /models/huggingface /app/logs

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
if [ -z "$HF_HOME" ]; then
    export HF_HOME="/models/huggingface"
fi

echo "üè† HuggingFace Home: $HF_HOME"

# –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∞)
echo "‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ $DEFAULT_MODEL..."

# –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞
echo "üöÄ –ó–∞–ø—É—Å–∫ vLLM OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞..."

exec python -m vllm.entrypoints.openai.api_server \
    --model "$DEFAULT_MODEL" \
    --served-model-name "$SERVED_MODEL_NAME" \
    --host "$HOST" \
    --port "$PORT" \
    --tensor-parallel-size "$TENSOR_PARALLEL_SIZE" \
    --gpu-memory-utilization "$GPU_MEMORY_UTILIZATION" \
    --max-model-len "$MAX_MODEL_LEN" \
    --max-num-seqs "$MAX_NUM_SEQS" \
    --block-size "$BLOCK_SIZE" \
    --swap-space "$SWAP_SPACE" \
    --dtype "$DTYPE" \
    --trust-remote-code \
    --enable-prefix-caching \
    --api-key "$API_KEY" \
    --disable-log-stats