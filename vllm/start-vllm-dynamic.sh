#!/bin/bash
# –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ vLLM —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π
# PDF Converter Pipeline v2.0 - –ò–°–ü–†–ê–í–õ–ï–ù–û

set -e

if [ -z "$VLLM_TENSOR_PARALLEL_SIZE" ] || [ -z "$VLLM_PIPELINE_PARALLEL_SIZE" ]; then
  echo "‚ùå Parallel sizes not set"
  exit 1
fi

echo "üöÄ –ó–∞–ø—É—Å–∫ Dynamic vLLM Server –¥–ª—è PDF Converter Pipeline v2.0"
echo "======================================================================"

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
HOST="${HOST:-0.0.0.0}"
PORT="${PORT:-8000}"
API_KEY="${VLLM_API_KEY:-pdf-converter-secure-key-2024}"

# GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 2x A6000
TENSOR_PARALLEL_SIZE="${VLLM_TENSOR_PARALLEL_SIZE:-2}"
GPU_MEMORY_UTILIZATION="${VLLM_GPU_MEMORY_UTILIZATION:-0.9}"

# –ú–æ–¥–µ–ª–∏ –¥–ª—è PDF Converter Pipeline
CONTENT_MODEL="${VLLM_MODEL_NAME:-Qwen/Qwen2.5-VL-32B-Instruct}"
TRANSLATION_MODEL="${VLLM_TRANSLATION_MODEL:-Qwen/Qwen3-30B-A3B-Instruct-2507}"

# –ü—É—Ç–∏
HF_HOME="${HF_HOME:-/models/huggingface}"
LOG_DIR="/app/logs"

echo "üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Dynamic vLLM Server:"
echo " Content Model: $CONTENT_MODEL"
echo " Translation Model: $TRANSLATION_MODEL" 
echo " Tensor Parallel: $TENSOR_PARALLEL_SIZE"
echo " GPU Memory Util: $GPU_MEMORY_UTILIZATION"
echo " Host: $HOST:$PORT"
echo " HuggingFace Cache: $HF_HOME"

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
echo "üìÅ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π..."
mkdir -p "$HF_HOME" "$LOG_DIR" /workspace/temp

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
echo "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏..."
if command -v nvidia-smi &> /dev/null; then
    echo "GPU Status:"
    nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv,noheader,nounits
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU
    GPU_COUNT=$(nvidia-smi --query-gpu=count --format=csv,noheader,nounits | head -1)
    echo "–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: $GPU_COUNT"
    
    if [ "$GPU_COUNT" -lt 2 ]; then
        echo "‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–µ–Ω—å—à–µ 2 GPU. Tensor Parallel –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ."
        TENSOR_PARALLEL_SIZE=1
    fi
else
    echo "‚ö†Ô∏è nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU."
fi

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è HuggingFace
export HF_HOME="$HF_HOME"
export HF_HUB_CACHE="$HF_HOME"
export TRANSFORMERS_CACHE="$HF_HOME"

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
export CUDA_VISIBLE_DEVICES="0,1"
export NCCL_DEBUG=WARN
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1

# vLLM —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
export VLLM_ATTENTION_BACKEND=FLASH_ATTN
export VLLM_USE_MODELSCOPE=false
export VLLM_WORKER_MULTIPROC_METHOD=spawn

# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è Model Manager
export VLLM_CONTENT_MODEL="$CONTENT_MODEL"
export VLLM_TRANSLATION_MODEL="$TRANSLATION_MODEL"
export VLLM_TENSOR_PARALLEL_SIZE="$TENSOR_PARALLEL_SIZE"
export VLLM_GPU_MEMORY_UTILIZATION="$GPU_MEMORY_UTILIZATION"

echo "üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π..."

# –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏ –≤ HuggingFace cache
check_model_availability() {
    local model_name=$1
    local model_dir="$HF_HOME/models--${model_name//\/--}"
    
    if [ -d "$model_dir" ]; then
        echo "‚úÖ –ú–æ–¥–µ–ª—å $model_name –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ"
        return 0
    else
        echo "üì• –ú–æ–¥–µ–ª—å $model_name –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ, –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"
        return 1
    fi
}

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–µ–π
echo "–ü—Ä–æ–≤–µ—Ä—è–µ–º Content Transformation –º–æ–¥–µ–ª—å..."
check_model_availability "$CONTENT_MODEL"

echo "–ü—Ä–æ–≤–µ—Ä—è–µ–º Translation –º–æ–¥–µ–ª—å..."
check_model_availability "$TRANSLATION_MODEL"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Python –º–æ–¥—É–ª–µ–π
echo "üêç –ü—Ä–æ–≤–µ—Ä–∫–∞ Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π..."
python -c "
try:
    import vllm
    print('‚úÖ vLLM:', vllm.__version__)
except ImportError:
    print('‚ùå vLLM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω')
    exit(1)

try:
    import torch
    print('‚úÖ PyTorch:', torch.__version__)
    print('‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞:', torch.cuda.is_available())
    if torch.cuda.is_available():
        print('‚úÖ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤:', torch.cuda.device_count())
except ImportError:
    print('‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω')
    exit(1)

try:
    import fastapi
    print('‚úÖ FastAPI:', fastapi.__version__)
except ImportError:
    print('‚ùå FastAPI –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω')
    exit(1)
"

if [ $? -ne 0 ]; then
    echo "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
    exit 1
fi

# –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Ä—Ç–æ–≤
check_port() {
    local port=$1
    if netstat -tuln | grep ":$port " > /dev/null 2>&1; then
        echo "‚ö†Ô∏è –ü–æ—Ä—Ç $port —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è"
        return 1
    else
        echo "‚úÖ –ü–æ—Ä—Ç $port —Å–≤–æ–±–æ–¥–µ–Ω"
        return 0
    fi
}

echo "üîå –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä—Ç–æ–≤..."
check_port "$PORT"

# –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è Model Manager
cat > /workspace/model_config.json << EOF
{
    "models": {
        "content_transformation": {
            "name": "$CONTENT_MODEL",
            "alias": "content-transformer",
            "task_type": "content_transformation",
            "estimated_vram_gb": 32.0,
            "tensor_parallel_size": $TENSOR_PARALLEL_SIZE,
            "max_model_len": 8192,
            "gpu_memory_utilization": $GPU_MEMORY_UTILIZATION
        },
        "translation": {
            "name": "$TRANSLATION_MODEL", 
            "alias": "translator",
            "task_type": "translation",
            "estimated_vram_gb": 30.0,
            "tensor_parallel_size": $TENSOR_PARALLEL_SIZE,
            "max_model_len": 8192,
            "gpu_memory_utilization": $GPU_MEMORY_UTILIZATION
        }
    },
    "server": {
        "host": "$HOST",
        "port": $PORT,
        "api_key": "$API_KEY"
    },
    "hardware": {
        "gpu_count": 2,
        "total_vram_gb": 96,
        "gpu_type": "NVIDIA A6000"
    }
}
EOF

echo "üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ /workspace/model_config.json"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
LOG_FILE="$LOG_DIR/vllm_dynamic_server.log"
touch "$LOG_FILE"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è graceful shutdown
cleanup() {
    echo "üîÑ –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏..."
    if [ ! -z "$VLLM_PID" ]; then
        echo "–û—Å—Ç–∞–Ω–æ–≤–∫–∞ vLLM —Å–µ—Ä–≤–µ—Ä–∞ (PID: $VLLM_PID)"
        kill -TERM "$VLLM_PID" 2>/dev/null || true
        wait "$VLLM_PID" 2>/dev/null || true
    fi
    echo "‚úÖ Dynamic vLLM Server –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
    exit 0
}

# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤
trap cleanup SIGTERM SIGINT SIGQUIT

echo "üöÄ –ó–∞–ø—É—Å–∫ Dynamic vLLM Server —Å Model Manager..."
echo "–õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: $LOG_FILE"
echo "======================================================================"

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PYTHONPATH –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
export PYTHONPATH="/workspace:$PYTHONPATH"

# –ó–∞–ø—É—Å–∫ –Ω–∞—à–µ–≥–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ –≤–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ vLLM
cd /workspace

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–∞—à–∏—Ö —Ñ–∞–π–ª–æ–≤
if [ ! -f "model_manager.py" ]; then
    echo "‚ùå –§–∞–π–ª model_manager.py –Ω–µ –Ω–∞–π–¥–µ–Ω!"
    exit 1
fi

if [ ! -f "dynamic_server.py" ]; then
    echo "‚ùå –§–∞–π–ª dynamic_server.py –Ω–µ –Ω–∞–π–¥–µ–Ω!"
    exit 1
fi

# –ó–∞–ø—É—Å–∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞
echo "‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫ Python Dynamic Server..."
python -m vllm.entrypoints.openai.api_server \
    --model="$CONTENT_MODEL" \
    --host="$HOST" \
    --port="$PORT" \
    --served-model-name="dynamic-model" \
    --disable-log-requests \
    --tensor-parallel-size="$TENSOR_PARALLEL_SIZE" \
    --pipeline-parallel-size="${VLLM_PIPELINE_PARALLEL_SIZE:-1}" \
    --gpu-memory-utilization="$GPU_MEMORY_UTILIZATION" \
    --max-model-len=8192 \
    --enable-prefix-caching \
    --trust-remote-code \
    --enable-chunked-prefill \
    --max-num-batched-tokens=2048 \
    --max-num-seqs=32 \
    --dtype="auto" \
    --api-key="$API_KEY" \
    2>&1 | tee "$LOG_FILE" &

VLLM_PID=$!

echo "üìä Dynamic vLLM Server –∑–∞–ø—É—â–µ–Ω —Å PID: $VLLM_PID"

# –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞
echo "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞..."
TIMEOUT=300  # 5 –º–∏–Ω—É—Ç –Ω–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
ELAPSED=0
HEALTH_URL="http://localhost:$PORT/health"
MODELS_URL="http://localhost:$PORT/v1/models"

while [ $ELAPSED -lt $TIMEOUT ]; do
    if curl -s -f "$HEALTH_URL" > /dev/null 2>&1; then
        echo "‚úÖ Dynamic vLLM Server –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!"
        echo "üåê Health check: $HEALTH_URL"
  
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        echo "üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π..."
        MODELS_RESPONSE=$(curl -s "http://localhost:$PORT/v1/models" 2>/dev/null || echo "")
        if [ -n "$MODELS_RESPONSE" ]; then
            MODELS_COUNT=$(echo "$MODELS_RESPONSE" | python -c "import sys, json; data=json.load(sys.stdin); print(len(data.get('data', [])))" 2>/dev/null || echo "0")
            echo "üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: $MODELS_COUNT"
            echo "üìã Models API: http://localhost:$PORT/v1/models"
        else
            echo "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"
        fi
  
  echo "üí¨ Chat API: http://localhost:$PORT/v1/chat/completions"
  echo "üìä Status API: http://localhost:$PORT/v1/models/status"
  break
fi
    
    if ! kill -0 "$VLLM_PID" 2>/dev/null; then
        echo "‚ùå Dynamic vLLM Server –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ"
        exit 1
    fi
    
    sleep 5
    ELAPSED=$((ELAPSED + 5))
    echo "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ... ($ELAPSED/$TIMEOUT —Å–µ–∫—É–Ω–¥)"
done

if [ $ELAPSED -ge $TIMEOUT ]; then
    echo "‚ùå –¢–∞–π–º-–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞"
    kill -TERM "$VLLM_PID" 2>/dev/null || true
    exit 1
fi

# –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ
echo ""
echo "üéØ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –†–ê–ë–û–¢–ï!"
echo "======================================================================"
echo "Dynamic vLLM Server –¥–ª—è PDF Converter Pipeline v2.0"
echo ""
echo "üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:"
echo "  - Content Transformation: $CONTENT_MODEL"
echo "  - Translation: $TRANSLATION_MODEL"
echo ""
echo "üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:"
echo "  - GPU: 2x A6000 (48GB VRAM total)"
echo "  - Tensor Parallel: $TENSOR_PARALLEL_SIZE"
echo "  - –ü–∞–º—è—Ç—å GPU: ${GPU_MEMORY_UTILIZATION}0%"
echo ""
echo "üåê API Endpoints:"
echo "  - Health: http://localhost:$PORT/health"
echo "  - Models: http://localhost:$PORT/v1/models"
echo "  - Chat: http://localhost:$PORT/v1/chat/completions"
echo "  - Status: http://localhost:$PORT/v1/models/status"
echo "  - Metrics: http://localhost:$PORT/metrics"
echo ""
echo "üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:"
echo "  - –õ–æ–≥–∏: $LOG_FILE"
echo "  - PID: $VLLM_PID"
echo "======================================================================"

# –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
wait "$VLLM_PID"